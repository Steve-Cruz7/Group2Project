# -*- coding: utf-8 -*-
"""Malware_MAML_MORSE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LNhnaeid5fUxnTM1ZYcI6K6Imv8T0GA_

# Malware classification using MAML.

  - Dataset: dataset from MORSE
  - Author: Jihoon Shin
  - Last Modified: Nov 15th, 2024

### Using MORSE dataset
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/Dataset/MORSE"

!ls

# Commented out IPython magic to ensure Python compatibility.
import pickle
import random
import numpy as np
import pandas as pd
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from collections import OrderedDict
from collections import Counter
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import copy

# %matplotlib inline

# import the dataset

# 1. synthetic malware dataset (synthetic folder - malware.npz)
loaded_data_syn = np.load('synthetic/malware.npz')
print(loaded_data_syn)
print(loaded_data_syn.files)

train_data_syn = loaded_data_syn['X_train']
train_label_syn = loaded_data_syn['y_train']
test_data_syn = loaded_data_syn['X_test']
test_label_syn = loaded_data_syn['y_test']

print(train_data_syn.shape) # (10000, 2381)
print(train_label_syn.shape)  # (10000,) label : 0 ~ 9
print(test_data_syn.shape) # (5000, 2381)
print(test_label_syn.shape) # (5000,)
print(f"unique_labels: {np.unique(train_label_syn)}") # get unique label
print("-------------------------------------------------------------")

# 2. synthetic malware dataset (synthetic folder - malware_true.npz)
loaded_data_syn_true = np.load('synthetic/malware_true.npz')
print(loaded_data_syn_true)
print(loaded_data_syn_true.files)

train_data_syn_true = loaded_data_syn_true['X_train']
train_label_syn_true = loaded_data_syn_true['y_train']
test_data_syn_true = loaded_data_syn_true['X_test']
test_label_syn_true = loaded_data_syn_true['y_test']

print(train_data_syn_true.shape) # (5500, 2381)
print(train_label_syn_true.shape) # (5500,) label : 0 ~ 9
print(test_data_syn_true.shape) # (5000, 2381)
print(test_label_syn_true.shape) # (5000,)
print(f"unique_labels: {np.unique(train_label_syn_true)}") # get unique label
print("-------------------------------------------------------------")

# 3. real world malware dataset (realworld folder - malware.npz)
loaded_data_real = np.load('real_world/malware.npz')
print(loaded_data_real)
print(loaded_data_real.files)

train_data_real = loaded_data_real['X_train']
train_label_real = loaded_data_real['y_train']
test_data_real = loaded_data_real['X_test']
test_label_real = loaded_data_real['y_test']

print(f"train_label_real: {train_label_real[:5]}")

print(train_data_real.shape)  # (5474, 1024)
print(train_label_real.shape) # (5474,) label: 0 ~ 11 -> 10, 11: blank or noise data?
print(test_data_real.shape) # (1200, 1024)
print(test_label_real.shape) # (1200,)
print(f"unique_labels: {np.unique(train_label_real)}") # get unique label
print("-------------------------------------------------------------")

# 4. real world malware dataset (realworld folder - malware_true.npz)
loaded_data_real_true = np.load('real_world/malware_true.npz')
print(loaded_data_real_true)
print(loaded_data_real_true.files)

train_data_real_true = loaded_data_real_true['X_train']
train_label_real_true = loaded_data_real_true['y_train']
test_data_real_true = loaded_data_real_true['X_test']
test_label_real_true = loaded_data_real_true['y_test']

print(train_data_real_true.shape) # (5474, 1024)
print(train_label_real_true.shape) # (5474,) label: 0 ~ 11
print(test_data_real_true.shape) # (1200, 1024)
print(test_label_real_true.shape) # (1200,)
print(f"unique_labels: {np.unique(train_label_real_true)}") # get unique label
print("-------------------------------------------------------------")

loaded_data = np.load('synthetic/malware.npz')  # or synthetic/malware_true.npz

# Extract data arrays
X_train = loaded_data['X_train']
y_train = loaded_data['y_train']
X_test = loaded_data['X_test']
y_test = loaded_data['y_test']

# Combine train and test data
data = np.concatenate((X_train, X_test), axis=0)
label = np.concatenate((y_train, y_test), axis=0)

# Verify the combined dataset shapes
print(f"Combined X shape: {data.shape}")
print(f"Combined y shape: {label.shape}")

"""# Code for MAML starts from here."""

# check the changed data
print(data.shape)
print(label)

class_samples = np.unique(label)

# Print the unique labels
print(f"Unique labels: {class_samples}")

# Normalize the features before training.
data = np.array(data)
scaler = StandardScaler() # Initialize the scaler
normalized_vector = scaler.fit_transform(data)

print(normalized_vector.shape)
print(normalized_vector[0])

data = normalized_vector  # We'll use normalized vector in the code.

# class for the Training Task & Testing Task
class MetaTrainingTask(object):
  def __init__(self, train_dataset, training_classes, num_sampled_class_training, num_sample_per_class, num_support_instances, num_query_instances, mode):
    self.train_dataset = train_dataset  # part of loaded_data which only has classes for training.
    self.num_classes = num_sampled_class_training  # number of classes in each task
    self.num_support_instances = num_support_instances  # number of instances per class in the support set
    self.num_query_instances = num_query_instances  # number of instances per class in the query set
    self.support_vectors = []
    self.support_labels = []
    self.query_vectors = []
    self.query_labels = []

    sampled_classes = random.sample(training_classes, self.num_classes)  # sample classes for training

    # Create a label encoder
    self.label_encoder = LabelEncoder()
    self.label_encoder.fit(sampled_classes)

    for sampled_label in sampled_classes:
      # samples = self.train_dataset[label] # take all samples from each label.
      samples = [item_vector for item_vector, item_label in self.train_dataset if item_label == sampled_label]

      if len(samples) < num_sample_per_class:
        raise ValueError(f"[MetaTraining] Not enough samples for class {label}, ({len(samples)}) / {num_sample_per_class}.")
      sample_idxs = np.random.choice(len(samples), num_sample_per_class, replace=False)  # randomly shuffle and get sample as num_sample_per_class

      # support_samples: used for training (inner loop in MAML).
      support_samples = sample_idxs[:self.num_support_instances]
      # query_samples: use to evaluate the model's performance on that specific task.
      query_samples = sample_idxs[self.num_support_instances:self.num_support_instances + num_query_instances]

      # Used in the inner loop.
      self.support_vectors.append(torch.tensor(np.array([samples[i].astype(float) for i in support_samples]), dtype=torch.float32))
      self.support_labels.extend([sampled_label] * len(support_samples))

      # Used in the outer loop (meta-training).
      self.query_vectors.append(torch.tensor(np.array([samples[i].astype(float) for i in query_samples]), dtype=torch.float32))
      self.query_labels.extend([sampled_label] * len(query_samples))

    # Convert lists to tensors
    self.support_vectors = torch.cat(self.support_vectors, dim=0)
    self.support_labels = torch.tensor(self.label_encoder.transform(self.support_labels), dtype=torch.long)
    self.query_vectors = torch.cat(self.query_vectors, dim=0)
    self.query_labels = torch.tensor(self.label_encoder.transform(self.query_labels), dtype=torch.long)

  def __len__(self):
    return len(self.query_labels)

  def __getitem__(self, idx):
    return self.query_vectors[idx], self.query_labels[idx]

# generates test tasks for evaluating the model after training.
class MetaTestingTask(object):
  def __init__(self, test_dataset, testing_classes, num_sampled_class_testing, num_sample_per_class, num_support_instances, num_query_instances, mode):
    self.test_dataset = test_dataset  # part of loaded_data which only has classes for testing.
    self.num_classes = num_sampled_class_testing
    self.num_support_instances = num_support_instances  # number of instances per class in the support set
    self.num_query_instances = num_query_instances  # number of instances per class in the query set
    self.support_vectors = []
    self.support_labels = []
    self.query_vectors = []
    self.query_labels = []

    sampled_classes = random.sample(testing_classes, self.num_classes)  # sample classes for testing

    # Create a label encoder
    self.label_encoder = LabelEncoder()
    self.label_encoder.fit(sampled_classes)

    for sampled_label in sampled_classes:
      samples = [item_vector for item_vector, item_label in self.test_dataset if item_label == sampled_label]

      if len(samples) < num_sample_per_class:
        raise ValueError(f"[MetaTesting] Not enough samples for class {sampled_label}, ({len(samples)}) / {num_sample_per_class}.")
      sample_idxs = np.random.choice(len(samples), num_sample_per_class, replace=False)  # randomly shuffle and select

      # support_samples: use to adapt the model to the new task.
      support_samples = sample_idxs[:self.num_support_instances]
      # query_samples: use to evaluate the model's performance on this task after adaptation.
      query_samples = sample_idxs[self.num_support_instances:self.num_support_instances + self.num_query_instances]

      # Used for quick adaptation in the testing phase.
      self.support_vectors.append(torch.tensor([samples[i].astype(float) for i in support_samples], dtype=torch.float32))
      self.support_labels.extend([sampled_label] * len(support_samples))

      # Used to evaluate the model's performance on new tasks.
      self.query_vectors.append(torch.tensor([samples[i].astype(float) for i in query_samples], dtype=torch.float32))
      self.query_labels.extend([sampled_label] * len(query_samples))

    # Convert lists to tensors
    self.support_vectors = torch.cat(self.support_vectors, dim=0)
    self.support_labels = torch.tensor(self.label_encoder.transform(self.support_labels), dtype=torch.long)
    self.query_vectors = torch.cat(self.query_vectors, dim=0)
    self.query_labels = torch.tensor(self.label_encoder.transform(self.query_labels), dtype=torch.long)

  def __len__(self):
    return len(self.query_labels)

  def __getitem__(self, idx):
    return self.query_vectors[idx], self.query_labels[idx]

''' BaseNet with 3-layers '''
class BaseNet(nn.Module):
  def __init__(self, num_classes):
    super(BaseNet, self).__init__()

    self.fc1 = nn.Linear(2381,128)
    self.fc2 = nn.Linear(128, 64)  # Additional hidden layer
    self.fc3 = nn.Linear(64, num_classes)  # Output layer

  def forward(self, x, weight=None):
    if weight is None:
            x = F.relu(self.fc1(x))  # Apply ReLU activation
            x = F.relu(self.fc2(x))
            x = self.fc3(x)  # Output layer
    else:
        x = F.linear(x, weight['fc1.weight'], weight['fc1.bias'])
        x = F.relu(x)
        x = F.linear(x, weight['fc2.weight'], weight['fc2.bias'])
        x = F.relu(x)
        x = F.linear(x, weight['fc3.weight'], weight['fc3.bias'])

    out = F.log_softmax(x, dim=1)
    return out

def clone_model(original_model):
  cloned_model = BaseNet(original_model.fc3.out_features)  # Create a new model instance
  cloned_model.load_state_dict(original_model.state_dict())  # Copy weights

  for cloned_param, original_param in zip(cloned_model.parameters(), original_model.parameters()):
    cloned_param.data = original_param.data  # Share the same underlying data
    cloned_param.grad = original_param.grad  # Share the gradient references
  return cloned_model

# Meta Learner
class MAML(nn.Module):
  def __init__(self, model, inner_lr, outer_lr, num_inner_steps, mode):
    super(MAML, self).__init__()
    self.model = model
    self.inner_lr = inner_lr  # learning rate for Inner loop updates
    self.outer_lr = outer_lr  # learning rate for Outer loop updates
    self.num_inner_steps = num_inner_steps  # number of steps for Inner loop updates
    self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.outer_lr)
    self.mode = mode

  # Inner Loop (Task-Specific Training)
  def inner_loop(self, support_vectors, support_labels):
    task_loss_fn = nn.CrossEntropyLoss()

    # Clone current weights (to avoid modifying the main model)
    temp_model = {name: param.clone() for name, param in self.model.named_parameters()}

    for _ in range(self.num_inner_steps):
      # Forward pass with fast weights
      preds = self.model(support_vectors, weight=temp_model)
      loss = task_loss_fn(preds, support_labels)

      # Compute gradients with respect to current fast weights
      grads = torch.autograd.grad(loss, temp_model.values(), create_graph=True)

      # Perform gradient update on fast weights
      temp_model = {name: param - self.inner_lr * grad for (name, param), grad in
                      zip(temp_model.items(), grads)}

    return temp_model  # Return the fast weights

  # Outer Loop (Meta-Update)
  def outer_loop(self, meta_tasks):
    total_loss = 0
    cnt = 0 # for log

    for task in meta_tasks:
      # [Module Test] Log the loss value
      if self.mode == 'verbose_maml':
        initial_predictions = self.model(task.support_vectors)
        initial_loss = F.cross_entropy(initial_predictions, task.support_labels)

      temp_model = self.inner_loop(task.support_vectors, task.support_labels)
      predictions = self.model(task.query_vectors, weight=temp_model)
      loss = F.cross_entropy(predictions, task.query_labels)

      total_loss += loss

    total_loss /= len(meta_tasks)

    self.optimizer.zero_grad()  # set the gradients to zero before starting to do backpropagation (i.e., updating the Weights and biases) because PyTorch accumulates the gradients on subsequent backward passes.
    total_loss.backward() # Compute gradients with respect to the main model's parameters
    self.optimizer.step() # applies the computed gradients to update the model's parameters

    return total_loss

  def evaluate(self, task):
    temp_model = self.inner_loop(task.support_vectors, task.support_labels)
    predictions = self.model(task.query_vectors, weight=temp_model) # new version
    loss = F.cross_entropy(predictions, task.query_labels)

    # compute accuracy
    predicted_labels = torch.argmax(predictions, dim=1)
    corrected_predictions = (predicted_labels == task.query_labels).sum().item()
    accuracy = corrected_predictions / len(task.query_labels)

    return loss.item(), accuracy

''' --- [Set the number of classes and Split the dataset] --- '''
num_classes = len(class_samples)  # Number of unique malware families

# Split the classes into Training and Testing
num_classes_training = 7  # Number of classes for training
num_classes_testing = num_classes - num_classes_training # Number of classes for testing

num_sampled_class_training = 5 # 5 classes will be sampled from the 7 (num_classes_training) classes in Training Task class.
num_sampled_class_testing = 2 # 2 classes will be sampled from the 2 (num_classes_testing) classes in Testing Task class.

num_samples_per_class = 30 # Number of samples per class
num_support_instances = 10  # Number of support instances per class
num_query_instances = 20  # Number of query instances per class

# Count how many samples each class has
class_sample_counts = Counter(label)
print(f"class_sample_counts: {class_sample_counts}")

# Filter classes that have at least num_sampled_class_training samples  --> Just for checking, "Not malware" data is not included.
eligible_training_classes = [cls for cls in class_sample_counts if class_sample_counts[cls] >= num_samples_per_class]
training_classes = random.sample(eligible_training_classes, num_classes_training) # dataset for the training
print(f"training_classes: {len(training_classes)}")

# Filter classes that are not in training_classes and have at least num_sampled_class_testing samples for testing
eligible_testing_classes = [cls for cls in class_sample_counts
                            if cls not in training_classes and class_sample_counts[cls] >= num_samples_per_class]

testing_classes = random.sample(eligible_testing_classes, num_classes_testing)
print(f"testing_classes: {len(testing_classes)}")

# Split the loaded_data into two group - data in training_classes / data in testing_class
meta_training_data = []
meta_testing_data = []

# Iterate through the entire dataset and split based on the label
for i in range(len(label)):
  if label[i] in training_classes:
    meta_training_data.append((data[i], label[i]))  # Add tuple (vector, label) to training data
  elif label[i] in testing_classes:
    meta_testing_data.append((data[i], label[i]))   # Add tuple (vector, label) to testing data

mode = 'normal'
inner_lr = 0.01  # Learning rate for inner loop
outer_lr = 0.001  # Learning rate for outer loop

num_meta_tasks = 5         # Number of meta-training & meta-testing tasks per epoch (Increased from 20)
num_inner_steps = 20       # Number of inner loop steps (Increased from 10)
num_epochs = 30             # Number of meta-training epochs (Increased from 100)

# Initialize the model and MAML
model = BaseNet(num_classes)
print(model)
maml = MAML(model, inner_lr, outer_lr, num_inner_steps, mode)

''' Checking the model. '''
for epoch in range(num_epochs):
  # generate meta-training tasks
  meta_tasks = [MetaTrainingTask(meta_training_data, training_classes, num_sampled_class_training, num_samples_per_class, num_support_instances, num_query_instances, mode) for _ in range(num_meta_tasks)]

  # perform meta-training
  meta_loss = maml.outer_loop(meta_tasks)
  print(f'Epoch {(epoch+1)}/{num_epochs}, Meta Loss: {meta_loss.item()}')

  # validation
  meta_testing_tasks = [MetaTestingTask(meta_testing_data, testing_classes, num_sampled_class_testing, num_samples_per_class, num_support_instances, num_query_instances, mode) for _ in range(num_meta_tasks)]
  total_test_loss = 0
  total_test_acc = 0

  for task in meta_testing_tasks:
    test_loss, test_acc = maml.evaluate(task)
    total_test_loss += test_loss
    total_test_acc += test_acc

  avg_test_loss = total_test_loss / len(meta_testing_tasks)
  avg_test_acc = total_test_acc / len(meta_testing_tasks)

  print(f'Epoch {(epoch+1)}/{num_epochs}, Validation Loss: {avg_test_loss}, Validation Accuracy: {avg_test_acc}')

print("Training Completed.")